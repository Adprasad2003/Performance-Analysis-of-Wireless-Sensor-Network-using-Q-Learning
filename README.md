# ${\color{red} Insurance\ Claim\ Analysis}$

# ‚úÖ ${\color{green} Objective}$
- To evaluate and compare the performance of AODV and DSDV routing protocols in Wireless Sensor Networks (WSNs) and analyze how Q-learning improves routing efficiency, energy usage, and network adaptability using metrics such as throughput, packet delivery ratio, delay, and packet drop.

# üõ†Ô∏è ${\color{grey} Tools\ Used}$
- NS2 (Network Simulator-2) for protocol simulation

- TCL Scripts for execution

- AWK Files for performance analysis

- NAM Files for visualization

- XG Files for graph generation
  
# üîÑ ${\color{blue} Steps\ Included}$
1. Creation of a static 24-node WSN topology

2. Implementation of AODV and DSDV routing protocols

3. Performance evaluation using NS2 trace files

4. Integration of Q-learning into routing

5. Reward computation and Q-table updates

6. Graph generation for delay, throughput, and jitter 

# üìä ${\color{orange} Analysis\ Included}$
- End-to-end delay comparison

- Throughput analysis

- Jitter evaluation

- Packet delivery and packet drop ratio

- Routing overhead performance

- Comparative evaluation of AODV, DSDV, and their Q-learning variants

# üîç ${\color{violet} Key\ Insights}$
- Q-learning enables dynamic route selection through exploration and exploitation.

- It reduces routing overhead in AODV and improves energy efficiency in DSDV.

- In static networks, Q-learning offers limited gains since optimal paths already exist.

- Better adaptability is observed under changing network conditions.

# üßæ ${\color{black} Conclusion}$
- The study confirms that Q-learning enhances adaptability, reduces overhead, and supports intelligent routing in WSNs. While performance gains in static networks are limited, Q-learning shows strong potential in dynamic environments. Future work suggests extending this approach to mobile networks and applying advanced models like Deep Q-Networks (DQN) for further optimization.
